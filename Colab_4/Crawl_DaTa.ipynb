{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Crawl_DaTa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bm_U06m87mQG"
      },
      "source": [
        "#**1.CÁC TRANG CHÂM BIẾM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_UjFI5D7yat"
      },
      "source": [
        "##The Onion"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfcrnVHk7g6U"
      },
      "source": [
        "import scrapy\n",
        "\n",
        "\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = 'onion'\n",
        "    def start_requests(self):\n",
        "        urls = [\n",
        "        ]\n",
        "        for i in range(0, 1001, 20):\n",
        "            urls.append('https://www.theonion.com/breaking-news/news-in-brief?startIndex=' + str(i))\n",
        "        \n",
        "        for url in urls:\n",
        "            yield scrapy.Request(url=url, callback=self.parse)\n",
        "    def parse(self, response):\n",
        "        for products in  response.css('a'):\n",
        "            yield {\n",
        "                \"article_link\":products.attrib['href'],\n",
        "                \"headline\": products.css('h2::text').get(),\n",
        "                \"is_sarcastic\": int(1),\n",
        "            }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvPZ5l-D715x"
      },
      "source": [
        "##New Jorker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1B5BRLtX8E_a"
      },
      "source": [
        "import scrapy\n",
        "\n",
        "\n",
        "class QuotesSpider(scrapy.Spider):\n",
        "    name = 'newjorker'\n",
        "    def start_requests(self):\n",
        "        urls = [\n",
        "            'https://www.newyorker.com/latest/news'\n",
        "        ]\n",
        "        for i in range(2,101):\n",
        "            urls.append('https://www.newyorker.com/latest/news/page/' + str(i))\n",
        "        \n",
        "        for url in urls:\n",
        "            yield scrapy.Request(url=url, callback=self.parse)\n",
        "    def parse(self, response):\n",
        "        for products in response.css('div.Layout__twoColumn___1sIWV').css('div.River__river___2W6ym').css('div.River__riverItemContent___2hXMG'):\n",
        "            try:\n",
        "                yield {\n",
        "                    \"article_link\": 'https://www.newyorker.com'+products.css('a').attrib['href'],\n",
        "                    \"headline\": products.css('h4::text').get(),\n",
        "                    \"is_sarcastic\": int(1),\n",
        "                }\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU-ch8GC8OMi"
      },
      "source": [
        "##The Poke"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6onz7pXU8KzH"
      },
      "source": [
        "import scrapy\n",
        "\n",
        "class PokeScrapy(scrapy.Spider):\n",
        "    name = 'poke'\n",
        "    start_urls = ['https://www.thepoke.co.uk/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        for newpap in response.css('article.boxgrid'):\n",
        "            try:\n",
        "                yield{\n",
        "                    \"article_link\": newpap.css('a').attrib['href'],\n",
        "                    \"headline\": newpap.css('p::text').get(),\n",
        "                    \"is_sarcastic\": int(1),\n",
        "                }\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "            next_page = response.css('div.prev').css('a').attrib['href']\n",
        "            if next_page is not None:\n",
        "                yield response.follow(next_page, callback = self.parse)\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuWVBO7k8eAE"
      },
      "source": [
        "##The HardTimes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijoJ1ZF48NQC"
      },
      "source": [
        "import scrapy\n",
        "\n",
        "class TheHardScrapy(scrapy.Spider):\n",
        "    name = 'thehardtimes'\n",
        "    start_urls = ['https://thehardtimes.net/']\n",
        "\n",
        "    def parse(self, response):\n",
        "        for newpap in response.css('div.post-header'):\n",
        "            try:\n",
        "                yield{\n",
        "                    \"article_link\": newpap.css('a').attrib['href'],\n",
        "                    \"headline\": newpap.css('a::text').get(),\n",
        "                    \"is_sarcastic\": int(1),\n",
        "                }\n",
        "            except:\n",
        "                continue\n",
        "        \n",
        "            next_page = response.css('a.next.page-numbers').attrib['href']\n",
        "            if next_page is not None:\n",
        "                yield response.follow(next_page, callback = self.parse)\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP5QsCnnwI2o"
      },
      "source": [
        "##Fortune"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UIk6sKQwTcr"
      },
      "source": [
        "import scrapy\n",
        "from ..items import FortunetutorialItem\n",
        "\n",
        "URL = ['https://fortune.com/tag/coronavirus/']\n",
        "for i in range(2, 260):\n",
        "    name = 'https://fortune.com/tag/coronavirus/page/' + str(i) + '/'\n",
        "    URL.append(name)\n",
        "\n",
        "class FortuneSpider(scrapy.Spider):\n",
        "    name = 'fortune'\n",
        "    start_urls = URL\n",
        "\n",
        "    def parse(self, response):\n",
        "        for news in response.css('li.termArchiveContentList__item--1LvxK'):\n",
        "            yield {\n",
        "                'header': news.css('div.termArchiveContentListItem__card--1he95 div.termArchiveContentListItem__title--14jcP a div::text').get(),\n",
        "                'link': news.css('div.termArchiveContentListItem__wrapper--1mGkD a::attr(href)').get(),\n",
        "            }\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDaR37Qowdij"
      },
      "source": [
        "##Canindia"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-zYQe4hwUHu"
      },
      "source": [
        "import scrapy\n",
        "from ..items import Canindiatutorial2Item\n",
        "\n",
        "URL = ['https://www.canindia.com/category/world/']\n",
        "for i in range(2, 1133):\n",
        "    name = 'https://www.canindia.com/category/world/page/' + str(i) + '/'\n",
        "    URL.append(name)\n",
        "\n",
        "class CanindiaWorldnewsSpider(scrapy.Spider):\n",
        "    name = 'canindia_worldnews'\n",
        "    start_urls = URL\n",
        "\n",
        "    def parse(self, response):\n",
        "        for news in response.css('div.td_module_10.td_module_wrap.td-animation-stack'):\n",
        "            yield {\n",
        "                'header': news.css('div.item-details h3.entry-title.td-module-title a::text').get(),\n",
        "                'link': news.css('div.item-details h3.entry-title.td-module-title a::attr(href)').get(),\n",
        "            }\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4EuwlbpwuxA"
      },
      "source": [
        "import scrapy\n",
        "from ..items import CanindiatutorialItem\n",
        "\n",
        "URL = ['https://www.canindia.com/category/sports/']\n",
        "for i in range(2, 1133):\n",
        "    name = 'https://www.canindia.com/category/sports/page/' + str(i) + '/'\n",
        "    URL.append(name)\n",
        "\n",
        "class CanindiaSportSpider(scrapy.Spider):\n",
        "    name = 'canindia_sport'\n",
        "    start_urls = URL\n",
        "\n",
        "    def parse(self, response):\n",
        "        for news in response.css('div.td_module_10.td_module_wrap.td-animation-stack'):\n",
        "            yield {\n",
        "                'header': news.css('div.item-details h3.entry-title.td-module-title a::text').get(),\n",
        "                'link': news.css('div.item-details h3.entry-title.td-module-title a::attr(href)').get(),\n",
        "            }\n",
        "        pass\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}